{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AlbertTokenizer, AlbertForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "model = AlbertForMaskedLM.from_pretrained('albert-base-v2')\n",
    "\n",
    "nlp = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer,topk=10)\n",
    "#print(nlp(f\"HuggingFace is creating a {nlp.tokenizer.mask_token} that the community uses to solve NLP tasks.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '[CLS] huggingface is creating a tool that the community uses to solve nlp tasks.[SEP]',\n",
       "  'score': 0.2723134756088257,\n",
       "  'token': 5607},\n",
       " {'sequence': '[CLS] huggingface is creating a platform that the community uses to solve nlp tasks.[SEP]',\n",
       "  'score': 0.06568177044391632,\n",
       "  'token': 2452},\n",
       " {'sequence': '[CLS] huggingface is creating a template that the community uses to solve nlp tasks.[SEP]',\n",
       "  'score': 0.0644591748714447,\n",
       "  'token': 22894},\n",
       " {'sequence': '[CLS] huggingface is creating a solution that the community uses to solve nlp tasks.[SEP]',\n",
       "  'score': 0.03637254238128662,\n",
       "  'token': 4295},\n",
       " {'sequence': '[CLS] huggingface is creating a resource that the community uses to solve nlp tasks.[SEP]',\n",
       "  'score': 0.035429175943136215,\n",
       "  'token': 6577},\n",
       " {'sequence': '[CLS] huggingface is creating a framework that the community uses to solve nlp tasks.[SEP]',\n",
       "  'score': 0.035174690186977386,\n",
       "  'token': 6596},\n",
       " {'sequence': '[CLS] huggingface is creating a strategy that the community uses to solve nlp tasks.[SEP]',\n",
       "  'score': 0.02455015480518341,\n",
       "  'token': 4427},\n",
       " {'sequence': '[CLS] huggingface is creating a mechanism that the community uses to solve nlp tasks.[SEP]',\n",
       "  'score': 0.02295648492872715,\n",
       "  'token': 6534},\n",
       " {'sequence': '[CLS] huggingface is creating a system that the community uses to solve nlp tasks.[SEP]',\n",
       "  'score': 0.021770361810922623,\n",
       "  'token': 329},\n",
       " {'sequence': '[CLS] huggingface is creating a methodology that the community uses to solve nlp tasks.[SEP]',\n",
       "  'score': 0.012585775926709175,\n",
       "  'token': 18653}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = nlp(\"HuggingFace is creating a [MASK] that the community uses to solve NLP tasks.\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tool'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = tokenizer.convert_ids_to_tokens(results[0]['token'])[1:]\n",
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2723134756088257"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = results[0]['score']\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlbertForMaskedLM(\n",
       "  (albert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (predictions): AlbertMLMHead(\n",
       "    (LayerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (dense): Linear(in_features=768, out_features=128, bias=True)\n",
       "    (decoder): Linear(in_features=128, out_features=30000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import logging\n",
    "\n",
    "\n",
    "def albert_masked_word_prediction(masked_examples, model, tokenizer, gpu_available, top_n, logger):\n",
    "#     if gpu_available:\n",
    "#         model.cuda()\n",
    "#         logger.info(\"successfully moved model to gpu\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    fill_mask_pipeline = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer, topk=top_n)\n",
    "    \n",
    "    avg_responses = {}\n",
    "    for j, key in enumerate(masked_examples):\n",
    "        binary_avg_score = 0\n",
    "        ratio_avg_score = 0\n",
    "\n",
    "        for example in masked_examples[key]:\n",
    "            statement, right_answer, wrong_answer = example\n",
    "            statement = statement.replace('<mask>','[MASK]')\n",
    "            print(statement)\n",
    "            responses = fill_mask_pipeline(statement)\n",
    "            right_pos = top_n + 1\n",
    "            wrong_pos = top_n + 1\n",
    "            right_score = 0\n",
    "            wrong_score = 0\n",
    "            done = -1\n",
    "            for i in range(len(responses)):\n",
    "                possible_answer = tokenizer.convert_ids_to_tokens(responses[0]['token'])[1:]\n",
    "                if possible_answer == right_answer:\n",
    "                    right_score = responses[i]['score']\n",
    "                    right_pos = i\n",
    "                    done += 1\n",
    "                if possible_answer == wrong_answer:\n",
    "                    wrong_score = responses[i]['score']\n",
    "                    wrong_pos = i\n",
    "                    done += 1\n",
    "                if done > 0:\n",
    "                    break\n",
    "            \n",
    "            binary_score = 1 if right_pos < wrong_pos else 0\n",
    "            \n",
    "            if right_score + wrong_score > 0:\n",
    "                ratio_score = (right_score - wrong_score) / (right_score + wrong_score)\n",
    "            else:\n",
    "                ratio_score = -1\n",
    "            \n",
    "            binary_avg_score += binary_score\n",
    "            ratio_avg_score += ratio_score\n",
    "\n",
    "        binary_avg_score /= float(len(masked_examples[key]))\n",
    "        ratio_avg_score /= float(len(masked_examples[key]))\n",
    "\n",
    "        avg_responses[key] = {\"binary_score\" : binary_avg_score, \"ratio_score\" : ratio_avg_score}\n",
    "        \n",
    "        if (j+1) % 240 == 0:\n",
    "            logger.info(\"finished 10 more\")\n",
    "\n",
    "    return avg_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joioaw is smaller than Ilkl, so Joioaw is [MASK] likely to fit into boxes than Ilkl.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0-original-original': {'binary_score': 0.0, 'ratio_score': -1.0}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(1012)\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "masked = {'0-original-original':[('Joioaw is smaller than Ilkl, so Joioaw is <mask> likely to fit into boxes than Ilkl.','more','less')]}\n",
    "\n",
    "albert_masked_word_prediction(masked,model,tokenizer,False,10,logger)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'statement' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-2429777616b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstatement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'statement' is not defined"
     ]
    }
   ],
   "source": [
    "statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (probing)",
   "language": "python",
   "name": "probing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
